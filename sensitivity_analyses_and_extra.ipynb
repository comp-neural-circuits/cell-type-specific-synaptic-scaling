{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a9175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# # Current directory is defined\n",
    "directory = os.getcwd() + \"/data/\"\n",
    "# os.chdir(directory)\n",
    "from model_analysis import *\n",
    "from plotting_functions import *\n",
    "\n",
    "# Directories for plotting are defined\n",
    "dir_data = directory\n",
    "dir_plot = directory + \"figures/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd93d38",
   "metadata": {},
   "source": [
    "# Data Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e347df8",
   "metadata": {},
   "source": [
    "#### For model with correct parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_analysis import *\n",
    "from plotting_functions import *\n",
    "\n",
    "# dir_data = '/home/ge74coy/mnt/nasgroup/labmembers/fabioveneto/synaptic_scaling/data/'\n",
    "dir_data = '/home/ge74coy/mnt/naspersonal/Code/synaptic_scaling/data/'\n",
    "# plot_all_cases_CIR(ww_weights,dir_data)\n",
    "# plot_all_cases_CIR_modified(ww_weights,dir_data)\n",
    "dir_plot = dir_data + \"figures/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edcdcd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#to open a single simulation\n",
    "K=0.25; g_top_down_to_S=0;\n",
    "flags = (1, 1, 1, 1, 1, 1)\n",
    "id, title = determine_name(flags)\n",
    "name_data = 'Case' + id + '_test_every_h' + '_k' + str(K).replace(\".\",\"\") + '_td' + str(g_top_down_to_S)\n",
    "with open(dir_data + name_data + '.pkl', 'rb') as file:\n",
    "    l_results = pickle.load(file)\n",
    "print('Data is read.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf30e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_cases_CIR(dir_data=dir_data, dir_plot=dir_plot + name_data, K=0.25, g_top_down_to_S=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ab2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check what it's imported\n",
    "hour_sims = np.arange(48) + 1\n",
    "l_all_delta_rE1 = []\n",
    "av_threshold_list = []\n",
    "name_plot = 'CIR_all_cases'\n",
    "\n",
    "flags_list = [(1, 1, 1, 1, 1, 1), (1, 1, 1, 0, 1, 1), (1, 1, 1, 1, 0, 1), (1, 1, 1, 1, 1, 0),\n",
    "              (1, 1, 1, 1, 0, 0), (1, 1, 1, 0, 1, 0), (1, 1, 1, 0, 0, 1)]\n",
    "\n",
    "for flags in flags_list:\n",
    "    id, title = determine_name(flags)\n",
    "    name_data = 'Case' + id + '_test_every_h' + '_k' + str(K).replace(\".\",\"\") + '_td' + str(g_top_down_to_S)\n",
    "#     name_data = 'Case' + id + '_test_every_h' + '_k' + str(K).replace(\".\",\"\") + '_td' + str(g_top_down_to_S) + \"_\"\n",
    "#     name_data += '_'.join(str(weight).replace(\".\",\"\") for weight in ww_weights)\n",
    "\n",
    "    # Open the file and read\n",
    "    with open(dir_data + name_data + '.pkl', 'rb') as file:\n",
    "        l_results = pickle.load(file)\n",
    "    print('Data is read.')\n",
    "\n",
    "    [_, l_time_points_phase2, _, l_delta_rE1, av_threshold, _, _] = l_results\n",
    "    l_all_delta_rE1.append(l_delta_rE1)\n",
    "    av_threshold_list.append(av_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a20d0",
   "metadata": {},
   "source": [
    "#### For the parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb41cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same function as in model_analysis.py BUT here you are opening fails based on their ww_weights. It is useful during the sensitivity analysis\n",
    "def plot_all_cases_CIR_modified(ww_weights,dir_data=r'\\figures\\data\\\\', dir_plot=r'\\figures\\\\', K=0.25, g_top_down_to_S=0):\n",
    "\n",
    "    \"\"\"\n",
    "    :param hour_sim: Defines how many hours does the simulation lasts\n",
    "    :param flags_list\n",
    "    :param flags_theta\n",
    "    :param dir_data\n",
    "    :param dir_plot\n",
    "    :param K: Tunes the steady state value of target activity and its regulator\n",
    "    :param g_top_down_to_S: Represents the top-down signal to SST neurons triggered by the hyperexcitation. It reaches\n",
    "    SST neurons at the offset of the conditioning\n",
    "    :param run_simulation: True to run the numerical simulation, False to read the already saved data\n",
    "    :param save_results: True to save the results\n",
    "    :param plot_results: True to plot the results\n",
    "\n",
    "    Multi-purpose function to analyze the model. Here we run (if run_simulation is True) our computational model to\n",
    "    investigate the role of cell-type dependent synaptic scaling mechanisms in associative learning. We replicate the\n",
    "    experimental procedure in [1] in model() in model.py. The model has two subnetworks, each consisted of a canonical\n",
    "    circuit of excitatory pyramidal neurons (E), parvalbumin-positive interneurons (P), somatostatin-positive\n",
    "    interneurons (S). The simulation procedure is divided into three phases:\n",
    "        Phase 1 - Conditioning: The first subnetwork receives extra input representing the conditioned stimulus in [1].\n",
    "        The parameters describing the stimulation (when and how much stimulation) is described in analyze_model()\n",
    "        function. The onset response of the excitatory firing rate of the first subnetwork is defined as the aversion\n",
    "        threshold of this network. Three-factor Hebbian learning is active during this period. Also, synaptic scaling\n",
    "        mechanisms, adaptive target activity (theta) and target activity regulator (beta) are active.\n",
    "\n",
    "        Phase 2: In the experiment [1], the novel stimulus is presented to the mice at 4h/24h/48h after conditioning.\n",
    "        This phase corresponds to the waiting time after conditioning and before testing. During this phase, synaptic\n",
    "        scaling mechanisms, adaptive target activity (theta) and target activity regulator (beta) are active.\n",
    "\n",
    "        Phase 3 - Testing: In this phase, the second subnetwork receives extra input corresponds to the novel stimulus\n",
    "        in [1]. The memory specificity/overgeneralization is determined whether the excitatory rate in the second\n",
    "        subnetwork is below/above the aversion threshold, respectively.\n",
    "\n",
    "\n",
    "    During simulation model() writes data to the data arrays. This data can be saved (if save_results is set to True)\n",
    "    and the results can be plotted (if plot_results is set to True).\n",
    "\n",
    "    [1] Wu, C. H., Ramos, R., Katz, D. B., & Turrigiano, G. G. (2021). Homeostatic synaptic scaling establishes the\n",
    "    specificity of an associative memory. Current biology, 31(11), 2274-2285.\n",
    "    \"\"\"\n",
    "    directory = \"\"\n",
    "\n",
    "    hour_sims = np.arange(48) + 1\n",
    "    l_all_delta_rE1 = []\n",
    "    name_plot = 'CIR_all_cases'\n",
    "\n",
    "    flags_list = [(1, 1, 1, 1, 1, 1), (1, 1, 1, 0, 1, 1), (1, 1, 1, 1, 0, 1), (1, 1, 1, 1, 1, 0),\n",
    "                  (1, 1, 1, 1, 0, 0), (1, 1, 1, 0, 1, 0), (1, 1, 1, 0, 0, 1)]\n",
    "\n",
    "    for flags in flags_list:\n",
    "        idd, title = determine_name(flags)\n",
    " #         name_data = 'Case' + id + '_test_every_h' + '_k' + str(K).replace(\".\",\"\") + '_td' + str(g_top_down_to_S)\n",
    "        name_data = 'Case' + idd + '_test_every_h' + '_k' + str(K).replace(\".\",\"\") + '_td' + str(g_top_down_to_S) + \"_\"\n",
    "        name_data += '_'.join(str(weight).replace(\".\",\"\") for weight in ww_weights)\n",
    " #         print(name_data)\n",
    "        \n",
    "        # Open the file and read\n",
    "        with open(directory+dir_data + name_data + '.pkl', 'rb') as file:\n",
    "            l_results = pickle.load(file)\n",
    "        print('Data is read.')\n",
    "\n",
    "        [_, l_time_points_phase2, _, l_delta_rE1, av_threshold, _, _] = l_results\n",
    " #         print(l_delta_rE1, \"\\n\")\n",
    "        l_all_delta_rE1.append(l_delta_rE1)\n",
    "\n",
    "        \n",
    "    print('Plotting the results.')\n",
    "    all_cases_CIR(l_time_points_phase2, hour_sims, l_all_delta_rE1, av_threshold, directory+dir_plot+name_plot, format='.png')\n",
    " #     all_cases_CIR(l_time_points_phase2, hour_sims, l_all_delta_rE1, av_threshold, directory+dir_plot+name_plot, format='.svg')\n",
    "\n",
    "\n",
    "def find_switch_index(arr):\n",
    "    for i, num in enumerate(arr):\n",
    "        if num < 0:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def print_ww_weights(ww_weights):\n",
    "    print(\"w_EE_within = \", ww_weights[4])\n",
    "    print(\"w_EE_cross = \", ww_weights[5])\n",
    "    print(\"w_EP_within = \", ww_weights[0])\n",
    "    print(\"w_EP_cross = \", ww_weights[1])\n",
    "    print(\"w_ES_within = \", ww_weights[2])\n",
    "    print(\"w_ES_cross = \", ww_weights[3])\n",
    "    \n",
    "def is_decreasing_from_peak(arr):\n",
    "    arr = np.array(arr)\n",
    "    # Find the index of the maximum value; this assumes the peak is the global maximum\n",
    "    peak_index = np.argmax(arr)\n",
    "    # Check if the array is monotonically decreasing after the peak\n",
    "    return np.all(arr[peak_index:-1] >= arr[peak_index+1:])\n",
    "\n",
    "def find_threshold_crossing(values, threshold):\n",
    "    \"\"\"\n",
    "    Finds the first index where the values list crosses below a given threshold.\n",
    "\n",
    "    Parameters:\n",
    "        values (list of float): The list of numerical values to check.\n",
    "        threshold (float): The threshold to check for crossings.\n",
    "\n",
    "    Returns:\n",
    "        int: The index at which the list crosses below the threshold, or None if it never crosses.\n",
    "    \"\"\"\n",
    "    values_array = np.array(values)\n",
    "    values_array[values_array == 0] = np.nan\n",
    "    \n",
    "    if all(values_array > baseline_reactivation) or any(np.isnan(values_array)):\n",
    "        if values_array[-1] - baseline_reactivation < baseline_reactivation/10: #all the values are above but it was very close to transition point\n",
    "            return len(values_array)-1\n",
    "        else:\n",
    "            return len(values_array)\n",
    "    \n",
    "\n",
    "    for i in range(1, len(values_array)):\n",
    "        # Check for crossing from above to below the threshold\n",
    "        if values_array[i - 1] >= threshold and values_array[i] < threshold:\n",
    "            # Find which of the two indices i-1 or i is closest to the threshold\n",
    "            diff_prev = abs(values_array[i - 1] - threshold)\n",
    "            diff_curr = abs(values_array[i] - threshold)\n",
    "            if diff_prev < diff_curr:\n",
    "                return i - 1\n",
    "            else:\n",
    "                return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cc80d3",
   "metadata": {},
   "source": [
    "### Import list of parameter sets that we used for simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c31f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can open all the paramters combination from the original file and save them in a list of lists\n",
    "file_path = '/home/ge74coy/mnt/naspersonal/Code/synaptic_scaling/param1.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    # Read lines from the file\n",
    "    lines = file.readlines()\n",
    "\n",
    "    # Process each line and create a list of parameter sets\n",
    "    parameter_sets = []\n",
    "    for line in lines:\n",
    "        # Split the line into individual floats using commas as separators\n",
    "        parameters = [float(value) for value in line.strip().split(',')]\n",
    "        parameter_sets.append(parameters)\n",
    "\n",
    "    # Print or process the parameter sets as needed\n",
    "#     for i, parameters in enumerate(parameter_sets, start=1):\n",
    "#         print(f\"Parameter set {i}: {parameters}\")\n",
    "    \n",
    "ww_weights = tuple(parameter_sets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random example\n",
    "ww_weights = tuple(parameter_sets[1223])\n",
    "dir_data = '/home/ge74coy/mnt/nasgroup/labmembers/fabioveneto/synaptic_scaling/data/'\n",
    "K=0.25; g_top_down_to_S=0;\n",
    "flags = (1, 1, 1, 0, 0, 1)\n",
    "id, title = determine_name(flags)\n",
    "name_data = 'Case' + id + '_test_every_h' + '_k' + str(K).replace(\".\",\"\") + '_td' + str(g_top_down_to_S) + \"_\"\n",
    "name_data += '_'.join(str(weight).replace(\".\",\"\") for weight in ww_weights)\n",
    "with open(dir_data + name_data + '.pkl', 'rb') as file:\n",
    "    l_results = pickle.load(file)\n",
    "print('Data is read.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb2ff63",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_data = '/home/ge74coy/mnt/nasgroup/labmembers/fabioveneto/synaptic_scaling/data/'\n",
    "ww_weights = tuple(parameter_sets[1833])\n",
    "K=0.25; g_top_down_to_S=0;\n",
    "print_ww_weights(ww_weights)\n",
    "plot_all_cases_CIR_modified(ww_weights,dir_data=dir_data, dir_plot=dir_plot, K=K, g_top_down_to_S=g_top_down_to_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99546078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING ALL THE SWITCH INDICES IN FULL NETWORK (we only want the simulations that had a switch from generic to specific within 48h)\n",
    "indices_tot = []\n",
    "for i in parameter_sets:\n",
    "    ww_weights = tuple(i)\n",
    "    \n",
    "    dir_data = '/home/ge74coy/mnt/nasgroup/labmembers/fabioveneto/synaptic_scaling/data/'\n",
    "    K=0.25; g_top_down_to_S=0;\n",
    "    flags = (1, 1, 1, 1, 1, 1)\n",
    "    id, title = determine_name(flags)\n",
    "    name_data = 'Case' + id + '_test_every_h' + '_k' + str(K).replace(\".\",\"\") + '_td' + str(g_top_down_to_S) + \"_\"\n",
    "    name_data += '_'.join(str(weight).replace(\".\",\"\") for weight in ww_weights)\n",
    "    with open(dir_data + name_data + '.pkl', 'rb') as file:\n",
    "        l_results = pickle.load(file)\n",
    "#     print('Data is read.')\n",
    "\n",
    "    [_, l_time_points_phase2, _, l_delta_rE1, av_threshold, _, _] = l_results\n",
    "    \n",
    "    baseline_reactivation = av_threshold / 1.15\n",
    "\n",
    "    change_in_reactivation = 100 * (np.array(l_delta_rE1) - baseline_reactivation) / baseline_reactivation\n",
    "\n",
    "#     print(change_in_reactivation)\n",
    "    switch_index = find_switch_index(change_in_reactivation)\n",
    "    indices_tot.append(switch_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784afbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the list of all the switches, extract the one that actually had a switch\n",
    "my_list = indices_tot\n",
    "\n",
    "non_zero_none_indices = [index for index, value in enumerate(my_list) if value is not None and value != 0]\n",
    "\n",
    "print(\"Indices where value is different from zero or None:\", non_zero_none_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2cc516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save list\n",
    "# with open('simulations_worked_indices.pkl', 'wb') as file:\n",
    "#     pickle.dump(my_list, file)\n",
    "\n",
    "#load list\n",
    "with open('simulations_worked_indices.pkl', 'rb') as file:\n",
    "    my_list = pickle.load(file)\n",
    "non_zero_none_indices = [index for index, value in enumerate(my_list) if value is not None and value != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cceabe",
   "metadata": {},
   "source": [
    "#### Trying to assess the difference between the generic and ayca's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f385260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open all flags for a certain weight set\n",
    "K=0.25;  g_top_down_to_S=0;\n",
    "dir_data = '/home/ge74coy/mnt/nasgroup/labmembers/fabioveneto/synaptic_scaling/data/'\n",
    "list_of_proper_indices_ayca = []\n",
    "list_of_proper_indices_gen = []\n",
    "list_of_proper_indices_ayca_proper = []\n",
    "list_of_proper_indices_free = []\n",
    "for index in non_zero_none_indices:\n",
    "    ww_weights = tuple(parameter_sets[index])\n",
    "    hour_sims = np.arange(48) + 1\n",
    "    l_all_delta_rE1 = []\n",
    "    av_threshold_list = []\n",
    "    name_plot = 'CIR_all_cases'\n",
    "\n",
    "    flags_list = [(1, 1, 1, 1, 1, 1), (1, 1, 1, 0, 1, 1), (1, 1, 1, 1, 0, 1), (1, 1, 1, 1, 1, 0),\n",
    "                  (1, 1, 1, 1, 0, 0), (1, 1, 1, 0, 1, 0), (1, 1, 1, 0, 0, 1)]\n",
    "\n",
    "    for flags in flags_list:\n",
    "        id, title = determine_name(flags)\n",
    "    #     name_data = 'Case' + id + '_test_every_h' + '_k' + str(K).replace(\".\",\"\") + '_td' + str(g_top_down_to_S)\n",
    "        name_data = 'Case' + id + '_test_every_h' + '_k' + str(K).replace(\".\",\"\") + '_td' + str(g_top_down_to_S) + \"_\"\n",
    "        name_data += '_'.join(str(weight).replace(\".\",\"\") for weight in ww_weights)\n",
    "\n",
    "        # Open the file and read\n",
    "        with open(dir_data + name_data + '.pkl', 'rb') as file:\n",
    "            l_results = pickle.load(file)\n",
    "        # print('Data is read.')\n",
    "\n",
    "        [_, l_time_points_phase2, _, l_delta_rE1, av_threshold, _, _] = l_results\n",
    "        l_all_delta_rE1.append(l_delta_rE1)\n",
    "        av_threshold_list.append(av_threshold)\n",
    "\n",
    "    for j in range(len(l_all_delta_rE1)):\n",
    "        if any(l_all_delta_rE1[j]) == 0:\n",
    "            l_all_delta_rE1[j] = np.full(len(l_all_delta_rE1[j]),np.NaN)\n",
    "            \n",
    "    closest_index = np.zeros(len(l_all_delta_rE1))\n",
    "    # Compute the absolute differences\n",
    "    baseline_reactivation = av_threshold_list[0] / 1.15\n",
    "    for i in range(len(l_all_delta_rE1)):\n",
    "        closest_index[i] = find_threshold_crossing(l_all_delta_rE1[i], baseline_reactivation)\n",
    "        if (closest_index[i] == len(l_all_delta_rE1[0])): #it's the condition inside searching for threshold function\n",
    "            closest_index[i] = np.NaN\n",
    "\n",
    "        \n",
    "    # EXPERIMENTAL CONDITIONS\n",
    "    if (closest_index[1] - closest_index[0] > 0):\n",
    "        list_of_proper_indices_gen.append(index)\n",
    "    # PREDICTED CONDITIONS\n",
    "    if (closest_index[1] - closest_index[0] > 0) and (np.isnan(closest_index[2])) and (closest_index[3] - closest_index[0] < 0) and (np.isnan(closest_index[4]) == False) and (np.isnan(closest_index[5]) == False) and (np.isnan(closest_index[6])):\n",
    "        list_of_proper_indices_ayca_proper.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc3924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save list\n",
    "with open('proper_indices_generic_conditions2.pkl', 'wb') as file:\n",
    "    pickle.dump(list_of_proper_indices_gen, file)\n",
    "with open('proper_indices_ayca_conditions3.pkl', 'wb') as file:\n",
    "    pickle.dump(list_of_proper_indices_ayca_proper, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82464c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can do the same and explore which parameter sets respect each condition\n",
    "K=0.25;  g_top_down_to_S=0;\n",
    "dir_data = '/home/ge74coy/mnt/nasgroup/labmembers/fabioveneto/synaptic_scaling/data/'\n",
    "list_of_proper_indices_difference_cond1 = []\n",
    "list_of_proper_indices_difference_cond2 = []\n",
    "list_of_proper_indices_difference_cond3 = []\n",
    "list_of_proper_indices_difference_cond4 = []\n",
    "list_of_proper_indices_difference_cond5 = []\n",
    "list_of_proper_indices_difference_cond6 = []\n",
    "list_of_proper_indices_difference_anyconditions = []\n",
    "for index in non_zero_none_indices:\n",
    "    ww_weights = tuple(parameter_sets[index])\n",
    "    hour_sims = np.arange(48) + 1\n",
    "    l_all_delta_rE1 = []\n",
    "    av_threshold_list = []\n",
    "    name_plot = 'CIR_all_cases'\n",
    "\n",
    "    flags_list = [(1, 1, 1, 1, 1, 1), (1, 1, 1, 0, 1, 1), (1, 1, 1, 1, 0, 1), (1, 1, 1, 1, 1, 0),\n",
    "                  (1, 1, 1, 1, 0, 0), (1, 1, 1, 0, 1, 0), (1, 1, 1, 0, 0, 1)]\n",
    "\n",
    "    for flags in flags_list:\n",
    "        id, title = determine_name(flags)\n",
    "    #     name_data = 'Case' + id + '_test_every_h' + '_k' + str(K).replace(\".\",\"\") + '_td' + str(g_top_down_to_S)\n",
    "        name_data = 'Case' + id + '_test_every_h' + '_k' + str(K).replace(\".\",\"\") + '_td' + str(g_top_down_to_S) + \"_\"\n",
    "        name_data += '_'.join(str(weight).replace(\".\",\"\") for weight in ww_weights)\n",
    "\n",
    "        # Open the file and read\n",
    "        with open(dir_data + name_data + '.pkl', 'rb') as file:\n",
    "            l_results = pickle.load(file)\n",
    "#         print('Data is read.')\n",
    "\n",
    "        [_, l_time_points_phase2, _, l_delta_rE1, av_threshold, _, _] = l_results\n",
    "        l_all_delta_rE1.append(l_delta_rE1)\n",
    "        av_threshold_list.append(av_threshold)\n",
    "\n",
    "    for j in range(len(l_all_delta_rE1)):\n",
    "        if any(l_all_delta_rE1[j]) == 0:\n",
    "            l_all_delta_rE1[j] = np.full(len(l_all_delta_rE1[j]),np.NaN)\n",
    "            \n",
    "    closest_index = np.zeros(len(l_all_delta_rE1))\n",
    "    # Compute the absolute differences\n",
    "    baseline_reactivation = av_threshold_list[0] / 1.15\n",
    "    for i in range(len(l_all_delta_rE1)):\n",
    "        closest_index[i] = find_threshold_crossing(l_all_delta_rE1[i], baseline_reactivation)\n",
    "        if (closest_index[i] == len(l_all_delta_rE1[0])): #it's the condition inside searching for threshold function\n",
    "            closest_index[i] = np.NaN\n",
    "     \n",
    "    flag = True \n",
    "    #ayca's conditions\n",
    "    if (closest_index[1] - closest_index[0] > 0):\n",
    "        list_of_proper_indices_difference_cond1.append(index)\n",
    "    if (np.isnan(closest_index[2])):\n",
    "        list_of_proper_indices_difference_cond2.append(index)\n",
    "        flag = False\n",
    "    if (closest_index[3] - closest_index[0] <= 0):\n",
    "        list_of_proper_indices_difference_cond3.append(index)\n",
    "        flag = False\n",
    "    if (closest_index[4] - closest_index[0] <= 0):\n",
    "        list_of_proper_indices_difference_cond4.append(index)\n",
    "        flag = False\n",
    "    if (closest_index[5] - closest_index[0] >= 0):\n",
    "        list_of_proper_indices_difference_cond5.append(index)\n",
    "        flag = False\n",
    "    if (np.isnan(closest_index[6])):\n",
    "        list_of_proper_indices_difference_cond6.append(index)\n",
    "        flag = False\n",
    "    if flag == True:\n",
    "        list_of_proper_indices_difference_anyconditions.append(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38035681",
   "metadata": {},
   "source": [
    "#### Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d372c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/ge74coy/mnt/naspersonal/Code/synaptic_scaling/param1.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    # Read lines from the file\n",
    "    lines = file.readlines()\n",
    "\n",
    "    # Process each line and create a list of parameter sets\n",
    "    parameter_sets = []\n",
    "    for line in lines:\n",
    "        # Split the line into individual floats using commas as separators\n",
    "        parameters = [float(value) for value in line.strip().split(',')]\n",
    "        parameter_sets.append(parameters)\n",
    "\n",
    "with open('simulations_worked_indices.pkl', 'rb') as file:\n",
    "    my_list = pickle.load(file)\n",
    "non_zero_none_indices = [index for index, value in enumerate(my_list) if value is not None and value != 0]\n",
    "\n",
    "with open('proper_indices_generic_conditions2.pkl', 'rb') as file:\n",
    "    list_of_proper_indices = pickle.load(file)\n",
    "# with open('proper_indices_ayca_conditions3.pkl', 'rb') as file:\n",
    "#     list_of_proper_indices = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3147b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "ww_weights = [tuple(parameter_sets[i]) for i in list_of_proper_indices]\n",
    "print_ww_weights(ww_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ccdaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ww_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4699b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "958/1075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bdb153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to plot difference between generic and ayca's free\n",
    "with open('proper_indices_generic_conditions2.pkl', 'rb') as file:\n",
    "    list_of_proper_indices_gen = pickle.load(file)\n",
    "with open('proper_indices_ayca_conditions3.pkl', 'rb') as file:\n",
    "    list_of_proper_indices_ayca = pickle.load(file)\n",
    "    \n",
    "ww_weights_ayca = [tuple(parameter_sets[i]) for i in list_of_proper_indices_ayca]\n",
    "ww_weights_gen = [tuple(parameter_sets[i]) for i in list_of_proper_indices_gen]\n",
    "\n",
    "# Convert lists to sets\n",
    "set1 = set(ww_weights_ayca)\n",
    "set2 = set(ww_weights_gen)\n",
    "\n",
    "# Find tuples in list2 not in list1\n",
    "unique_to_list = set2 - set1\n",
    "\n",
    "ww_weights = list(unique_to_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a1f3b6",
   "metadata": {},
   "source": [
    "Plot pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to plot difference between generic and ayca's free\n",
    "with open('proper_indices_generic_conditions2.pkl', 'rb') as file:\n",
    "    list_of_proper_indices_gen = pickle.load(file)\n",
    "with open('proper_indices_ayca_conditions3.pkl', 'rb') as file:\n",
    "    list_of_proper_indices_ayca = pickle.load(file)\n",
    "    \n",
    "ww_weights_ayca = [tuple(parameter_sets[i]) for i in list_of_proper_indices_ayca]\n",
    "ww_weights_gen = [tuple(parameter_sets[i]) for i in list_of_proper_indices_gen]\n",
    "\n",
    "# Convert lists to sets\n",
    "set1 = set(ww_weights_ayca)\n",
    "set2 = set(ww_weights_gen)\n",
    "\n",
    "# Find tuples in list2 not in list1\n",
    "unique_to_list = set2 - set1\n",
    "\n",
    "ww_weights_sol = list(set1)\n",
    "ww_weights_all = list(set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e69c037",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming ww_weights_sol and ww_weights_all are already defined as lists of tuples\n",
    "\n",
    "# Convert lists of tuples to DataFrames\n",
    "df1 = pd.DataFrame(\n",
    "    ww_weights_sol, \n",
    "    columns=[\"w_EP_within\", \"w_EP_cross\", \"w_ES_within\", \"w_ES_cross\", \"w_EE_within\", \"w_EE_cross\"]\n",
    ")\n",
    "df2 = pd.DataFrame(\n",
    "    ww_weights_all, \n",
    "    columns=[\"w_EP_within\", \"w_EP_cross\", \"w_ES_within\", \"w_ES_cross\", \"w_EE_within\", \"w_EE_cross\"]\n",
    ")\n",
    "\n",
    "# Add a 'Dataset' column to distinguish between df1 and df2\n",
    "df1['Dataset'] = 'Solution Weights'\n",
    "df2['Dataset'] = 'All Weights'\n",
    "\n",
    "# Combine the two DataFrames\n",
    "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Define a more distinct color palette\n",
    "palette = sns.color_palette(\"Set2\", n_colors=2)  # \"Set2\" provides soft, distinct colors\n",
    "\n",
    "# Set the Seaborn theme with a suitable context\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\")\n",
    "\n",
    "# Create the pair plot with KDE and hue to differentiate datasets\n",
    "pairplot = sns.pairplot(\n",
    "    df_combined,\n",
    "    hue='Dataset',\n",
    "    kind='kde',\n",
    "    diag_kind='kde',\n",
    "    diag_kws={'alpha': 0.7, 'fill': True, 'linewidths': 1},  # Changed 'linewidth' to 'linewidths'\n",
    "    plot_kws={'alpha': 0.6, 'linewidths': 1},              # Changed 'linewidth' to 'linewidths'\n",
    "    palette=palette,\n",
    "    height=3,    # Increased height for better visibility\n",
    "    aspect=1.2,   # Maintains the aspect ratio\n",
    "    corner=False  # Include the full pair plot\n",
    ")\n",
    "\n",
    "# Adjust plot spacing to prevent overlapping elements\n",
    "plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, hspace=0.2, wspace=0.2)\n",
    "\n",
    "# Set the same limits and ticks for all axes\n",
    "variables = df1.columns[:-1]  # Exclude the 'Dataset' column\n",
    "for i, var_y in enumerate(variables):\n",
    "    for j, var_x in enumerate(variables):\n",
    "        if i >= j:\n",
    "            ax = pairplot.axes[i, j]\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_xticks([0.0, 0.5, 1.0])\n",
    "            ax.set_yticks([0.0, 0.5, 1.0])\n",
    "            ax.tick_params(axis='both', labelsize=10)\n",
    "            \n",
    "            # Add gridlines to diagonal plots for better readability\n",
    "            if i == j:\n",
    "                ax.grid(True, linestyle='--', alpha=0.5)\n",
    "            else:\n",
    "                ax.grid(False)\n",
    "            \n",
    "            # Improve spine visibility\n",
    "            sns.despine(ax=ax, trim=True)\n",
    "\n",
    "# Adjust overall aesthetics\n",
    "pairplot.fig.suptitle('Pair Plot of Weights: Solution vs All', fontsize=18, y=1.02)  # Add a title with increased font size\n",
    "\n",
    "# Customize the legend\n",
    "pairplot.add_legend(title='Dataset', label_order=['Solution Weights', 'All Weights'], fontsize=12, title_fontsize=14)\n",
    "\n",
    "# Optional: Save the figure with higher resolution\n",
    "plt.savefig(\"enhanced_pairplot_comparison.pdf\", bbox_inches='tight')\n",
    "plt.savefig(\"enhanced_pairplot_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c91cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold_crossing(values, threshold):\n",
    "    \"\"\"\n",
    "    Finds the first index where the values list crosses below a given threshold.\n",
    "\n",
    "    Parameters:\n",
    "        values (list of float): The list of numerical values to check.\n",
    "        threshold (float): The threshold to check for crossings.\n",
    "\n",
    "    Returns:\n",
    "        int: The index at which the list crosses below the threshold, or None if it never crosses.\n",
    "    \"\"\"\n",
    "    values_array = np.array(values)\n",
    "    values_array[values_array == 0] = np.nan\n",
    "    \n",
    "    if all(values_array > baseline_reactivation) or any(np.isnan(values_array)):\n",
    "        if values_array[-1] - baseline_reactivation < baseline_reactivation/10: #all the values are above but it was very close to transition point\n",
    "            return len(values_array)-1\n",
    "        else:\n",
    "            return len(values_array)\n",
    "    \n",
    "\n",
    "    for i in range(1, len(values_array)):\n",
    "        # Check for crossing from above to below the threshold\n",
    "        if values_array[i - 1] >= threshold and values_array[i] < threshold:\n",
    "            # Find which of the two indices i-1 or i is closest to the threshold\n",
    "            diff_prev = abs(values_array[i - 1] - threshold)\n",
    "            diff_curr = abs(values_array[i] - threshold)\n",
    "            if diff_prev < diff_curr:\n",
    "                return i - 1\n",
    "            else:\n",
    "                return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7519add9",
   "metadata": {},
   "source": [
    "# CIR as function of time for all conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d0c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can open all the paramters combination from the original file and save them in a list of lists\n",
    "file_path = '/home/ge74coy/mnt/naspersonal/Code/synaptic_scaling/param1.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    # Read lines from the file\n",
    "    lines = file.readlines()\n",
    "\n",
    "    # Process each line and create a list of parameter sets\n",
    "    parameter_sets = []\n",
    "    for line in lines:\n",
    "        # Split the line into individual floats using commas as separators\n",
    "        parameters = [float(value) for value in line.strip().split(',')]\n",
    "        parameter_sets.append(parameters)\n",
    "\n",
    "    # Print or process the parameter sets as needed\n",
    "#     for i, parameters in enumerate(parameter_sets, start=1):\n",
    "#         print(f\"Parameter set {i}: {parameters}\")\n",
    "    \n",
    "ww_weights = tuple(parameter_sets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save list\n",
    "# with open('proper_indices_generic_conditions.pkl', 'wb') as file:\n",
    "#     pickle.dump(list_of_proper_indices, file)\n",
    "\n",
    "#load list (generic)\n",
    "with open('proper_indices_generic_conditions2.pkl', 'rb') as file:\n",
    "    list_of_proper_indices = pickle.load(file)\n",
    "\n",
    "#load list (ayca's)\n",
    "# with open('proper_indices_aycas_conditions3.pkl', 'rb') as file:\n",
    "#     list_of_proper_indices = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2511f8a0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot all of them in a certain range to qualitetively assess how they are\n",
    "hour_sims = np.arange(48) + 1\n",
    "\n",
    "flagss = [flags_list[0], flags_list[1]]\n",
    "for index in range(750,800):\n",
    "    print(index)\n",
    "    for flags in flagss:\n",
    "        ww_weights = tuple(parameter_sets[list_of_proper_indices[index]])\n",
    "        id, title = determine_name(flags)\n",
    "        # name_data = 'Case' + id + '_test_every_h' + '_k' + str(K).replace(\".\",\"\") + '_td' + str(g_top_down_to_S)\n",
    "        name_data = 'Case' + id + '_test_every_h' + '_k' + str(K).replace(\".\",\"\") + '_td' + str(g_top_down_to_S) + \"_\"\n",
    "        name_data += '_'.join(str(weight).replace(\".\",\"\") for weight in ww_weights)\n",
    "\n",
    "        # Open the file and read\n",
    "        with open(dir_data + name_data + '.pkl', 'rb') as file:\n",
    "            l_results = pickle.load(file)\n",
    "        print('Data is read.')\n",
    "\n",
    "        [_, l_time_points_phase2, _, l_delta_rE1, av_threshold, _, _] = l_results\n",
    "\n",
    "        change_in_reactivation_every_h_v3(l_time_points_phase2, hour_sims, l_delta_rE1, av_threshold,\n",
    "                                   dir_plot + name_data, flag_only_S_on=False, format='.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4dadf0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot all of them\n",
    "hour_sims = np.arange(48) + 1\n",
    "K=0.25; g_top_down_to_S=0\n",
    "dir_data = '/home/ge74coy/mnt/nasgroup/labmembers/fabioveneto/synaptic_scaling/data/'\n",
    "\n",
    "flags_list = [(1, 1, 1, 1, 1, 1), (1, 1, 1, 0, 1, 1), (1, 1, 1, 1, 0, 1), (1, 1, 1, 1, 1, 0),\n",
    "              (1, 1, 1, 1, 0, 0), (1, 1, 1, 0, 1, 0), (1, 1, 1, 0, 0, 1), (1, 1, 1, 0, 0, 0)]\n",
    "\n",
    "\n",
    "flagss = [flags_list[1]]\n",
    "# index=717\n",
    "index = 765\n",
    "for flags in flagss:\n",
    "    ww_weights = tuple(parameter_sets[list_of_proper_indices[index]])\n",
    "    id, title = determine_name(flags)\n",
    "    # name_data = 'Case' + id + '_test_every_h' + '_k' + str(K).replace(\".\",\"\") + '_td' + str(g_top_down_to_S)\n",
    "    name_data = 'Case' + id + '_test_every_h' + '_k' + str(K).replace(\".\",\"\") + '_td' + str(g_top_down_to_S) + \"_\"\n",
    "    name_data += '_'.join(str(weight).replace(\".\",\"\") for weight in ww_weights)\n",
    "\n",
    "    # Open the file and read\n",
    "    with open(dir_data + name_data + '.pkl', 'rb') as file:\n",
    "        l_results = pickle.load(file)\n",
    "    print('Data is read.')\n",
    "\n",
    "    [_, l_time_points_phase2, _, l_delta_rE1, av_threshold, _, _] = l_results\n",
    "\n",
    "    change_in_reactivation_every_h_v3(l_time_points_phase2, hour_sims, l_delta_rE1, av_threshold,\n",
    "                               dir_plot + name_data, flag_only_S_on=False, format='.png')\n",
    "    change_in_reactivation_every_h_v3(l_time_points_phase2, hour_sims, l_delta_rE1, av_threshold,\n",
    "                           dir_plot + name_data, flag_only_S_on=False, format='.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addfca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_ww_weights(ww_weights)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
